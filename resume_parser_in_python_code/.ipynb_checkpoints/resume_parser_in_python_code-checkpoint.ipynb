{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import logging\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import spacy\n",
    "from spacy.gold import GoldParse\n",
    "from spacy.scorer import Scorer\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataturks_to_spacy(dataturks_JSON_FilePath):\n",
    "    try:\n",
    "        training_data = []\n",
    "        lines=[]\n",
    "        with open(dataturks_JSON_FilePath, encoding=\"utf8\") as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            data = json.loads(line)\n",
    "            text = data['content']\n",
    "            entities = []\n",
    "            for annotation in data['annotation']:\n",
    "                #only a single point in text annotation.\n",
    "                point = annotation['points'][0]\n",
    "                labels = annotation['label']\n",
    "                # handle both list of labels or a single label.\n",
    "                if not isinstance(labels, list):\n",
    "                    labels = [labels]\n",
    "\n",
    "                for label in labels:\n",
    "                    #dataturks indices are both inclusive [start, end] but spacy is not [start, end)\n",
    "                    entities.append((point['start'], point['end'] + 1 ,label))\n",
    "\n",
    "\n",
    "            training_data.append((text, {\"entities\" : entities}))\n",
    "\n",
    "        return training_data\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Unable to process \" + dataturks_JSON_FilePath + \"\\n\" + \"error = \" + str(e))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### Train Spacy NER.###########\n",
    "def train_spacy():\n",
    "\n",
    "    TRAIN_DATA = convert_dataturks_to_spacy(\"traindata.json\")\n",
    "    nlp = spacy.blank('en')  # create blank Language class\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "       \n",
    "\n",
    "    # add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "         for ent in annotations.get('entities'):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(10):\n",
    "            print(\"Statring iteration \" + str(itn))\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            for text, annotations in TRAIN_DATA:\n",
    "                nlp.update(\n",
    "                    [text],  # batch of texts\n",
    "                    [annotations],  # batch of annotations\n",
    "                    drop=0.2,  # dropout - make it harder to memorise data\n",
    "                    sgd=optimizer,  # callable to update weights\n",
    "                    losses=losses)\n",
    "            print(losses)\n",
    "    #test the model and evaluate it\n",
    "    examples = convert_dataturks_to_spacy(\"testdata.json\")\n",
    "    tp=0\n",
    "    tr=0\n",
    "    tf=0\n",
    "\n",
    "    ta=0\n",
    "    c=0        \n",
    "    for text,annot in examples:\n",
    "\n",
    "        f=open(\"resume\"+str(c)+\".txt\",\"w\", encoding='utf-8')\n",
    "        doc_to_test=nlp(text)\n",
    "        d={}\n",
    "        for ent in doc_to_test.ents:\n",
    "            d[ent.label_]=[]\n",
    "        for ent in doc_to_test.ents:\n",
    "            d[ent.label_].append(ent.text)\n",
    "\n",
    "        for i in set(d.keys()):\n",
    "\n",
    "            f.write(\"\\n\\n\")\n",
    "            f.write(i +\":\"+\"\\n\")\n",
    "            for j in set(d[i]):\n",
    "                f.write(j.replace('\\n','')+\"\\n\")\n",
    "        d={}\n",
    "        for ent in doc_to_test.ents:\n",
    "            d[ent.label_]=[0,0,0,0,0,0]\n",
    "        for ent in doc_to_test.ents:\n",
    "            doc_gold_text= nlp.make_doc(text)\n",
    "            gold = GoldParse(doc_gold_text, entities=annot.get(\"entities\"))\n",
    "            y_true = [ent.label_ if ent.label_ in x else 'Not '+ent.label_ for x in gold.ner]\n",
    "            y_pred = [x.ent_type_ if x.ent_type_ ==ent.label_ else 'Not '+ent.label_ for x in doc_to_test]  \n",
    "            if(d[ent.label_][0]==0):\n",
    "                #f.write(\"For Entity \"+ent.label_+\"\\n\")   \n",
    "                #f.write(classification_report(y_true, y_pred)+\"\\n\")\n",
    "                (p,r,f,s)= precision_recall_fscore_support(y_true,y_pred,average='weighted')\n",
    "                a=accuracy_score(y_true,y_pred)\n",
    "                d[ent.label_][0]=1\n",
    "                d[ent.label_][1]+=p\n",
    "                d[ent.label_][2]+=r\n",
    "                d[ent.label_][3]+=f\n",
    "                d[ent.label_][4]+=a\n",
    "                d[ent.label_][5]+=1\n",
    "        c+=1\n",
    "    for i in d:\n",
    "        print(\"\\n For Entity \"+i+\"\\n\")\n",
    "        print(\"Accuracy : \"+str((d[i][4]/d[i][5])*100)+\"%\")\n",
    "        print(\"Precision : \"+str(d[i][1]/d[i][5]))\n",
    "        print(\"Recall : \"+str(d[i][2]/d[i][5]))\n",
    "        print(\"F-score : \"+str(d[i][3]/d[i][5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Unnamed vectors -- this won't allow multiple vectors models to be loaded. (Shape: (0, 0))\n",
      "Statring iteration 0\n",
      "{'ner': 6457.626186404118}\n",
      "Statring iteration 1\n",
      "{'ner': 3922.7332458961623}\n",
      "Statring iteration 2\n",
      "{'ner': 3380.6075062233976}\n",
      "Statring iteration 3\n",
      "{'ner': 2935.8592867321286}\n",
      "Statring iteration 4\n",
      "{'ner': 2591.141894535311}\n",
      "Statring iteration 5\n",
      "{'ner': 2285.3888324927107}\n",
      "Statring iteration 6\n",
      "{'ner': 2152.1570752364855}\n",
      "Statring iteration 7\n",
      "{'ner': 1923.178305102996}\n",
      "Statring iteration 8\n",
      "{'ner': 1746.272026233502}\n",
      "Statring iteration 9\n",
      "{'ner': 1649.7589684202035}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " For Entity Name\n",
      "\n",
      "Accuracy : 99.83805668016194%\n",
      "Precision : 0.9983831936194594\n",
      "Recall : 0.9983805668016195\n",
      "F-score : 0.9981113185060555\n",
      "\n",
      " For Entity Location\n",
      "\n",
      "Accuracy : 99.27125506072875%\n",
      "Precision : 0.9927657005623397\n",
      "Recall : 0.9927125506072875\n",
      "F-score : 0.9897446574315648\n",
      "\n",
      " For Entity Email Address\n",
      "\n",
      "Accuracy : 99.43319838056681%\n",
      "Precision : 1.0\n",
      "Recall : 0.994331983805668\n",
      "F-score : 0.9971579374746244\n",
      "\n",
      " For Entity Companies worked at\n",
      "\n",
      "Accuracy : 98.78542510121457%\n",
      "Precision : 1.0\n",
      "Recall : 0.9878542510121457\n",
      "F-score : 0.9938900203665988\n",
      "\n",
      " For Entity Designation\n",
      "\n",
      "Accuracy : 99.83805668016194%\n",
      "Precision : 1.0\n",
      "Recall : 0.9983805668016195\n",
      "F-score : 0.9991896272285252\n",
      "\n",
      " For Entity College Name\n",
      "\n",
      "Accuracy : 100.0%\n",
      "Precision : 1.0\n",
      "Recall : 1.0\n",
      "F-score : 1.0\n",
      "\n",
      " For Entity Skills\n",
      "\n",
      "Accuracy : 100.0%\n",
      "Precision : 1.0\n",
      "Recall : 1.0\n",
      "F-score : 1.0\n"
     ]
    }
   ],
   "source": [
    "train_spacy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import spacy\n",
    "import sys \n",
    "from importlib import reload\n",
    "reload(sys)\n",
    "import pandas as pd\n",
    "#sys.setdefaultencoding('utf-8')\n",
    "from io import StringIO\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from spacy.lang.xx import MultiLanguage\n",
    "import os\n",
    "import sys, getopt\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function converting pdf to string\n",
    "def convert(fname, pages=None):\n",
    "    if not pages:\n",
    "        pagenums = set()\n",
    "    else:\n",
    "        pagenums = set(pages)\n",
    "\n",
    "    output = StringIO()\n",
    "    manager = PDFResourceManager()\n",
    "    converter = TextConverter(manager, output, laparams=LAParams())\n",
    "    interpreter = PDFPageInterpreter(manager, converter)\n",
    "    \n",
    "    \n",
    "    infile = open(fname, 'rb')\n",
    "    \n",
    "    \n",
    "    for page in PDFPage.get_pages(infile, pagenums):\n",
    "        interpreter.process_page(page)\n",
    "    infile.close()\n",
    "    converter.close()\n",
    "    text = output.getvalue()\n",
    "    output.close\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\ndef convert(fname, pages=None):\\n    if not pages:\\n        pagenums = set()\\n    else:\\n        pagenums = set(pages)\\n\\n    output = StringIO()\\n    manager = PDFResourceManager()\\n    converter = TextConverter(manager, output, laparams=LAParams())\\n    interpreter = PDFPageInterpreter(manager, converter)\\n\\n    infile = file(\"resume.pdf\", \\'rb\\')\\n    for page in PDFPage.get_pages(infile, pagenums):\\n        interpreter.process_page(page)\\n    infile.close()\\n    converter.close()\\n    text = output.getvalue()\\n    output.close\\n\\n    with open(\\'1.txt\\', \\'w\\') as pdf_file:\\n        pdf_file.write(text)\\n\\n    return text\\n    \\n'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "def convert(fname, pages=None):\n",
    "    if not pages:\n",
    "        pagenums = set()\n",
    "    else:\n",
    "        pagenums = set(pages)\n",
    "\n",
    "    output = StringIO()\n",
    "    manager = PDFResourceManager()\n",
    "    converter = TextConverter(manager, output, laparams=LAParams())\n",
    "    interpreter = PDFPageInterpreter(manager, converter)\n",
    "\n",
    "    infile = file(\"resume.pdf\", 'rb')\n",
    "    for page in PDFPage.get_pages(infile, pagenums):\n",
    "        interpreter.process_page(page)\n",
    "    infile.close()\n",
    "    converter.close()\n",
    "    text = output.getvalue()\n",
    "    output.close\n",
    "\n",
    "    with open('1.txt', 'w') as pdf_file:\n",
    "        pdf_file.write(text)\n",
    "\n",
    "    return text\n",
    "    \n",
    "\"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to extract names from the string using spacy\n",
    "def extract_name(string):\n",
    "    r1 = str(string)\n",
    "    nlp = MultiLanguage()\n",
    "    doc = nlp(r1)\n",
    "    for ent in doc.ents:\n",
    "        if(ent.label_ == 'PER'):\n",
    "            print(ent.text)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to extract Phone Numbers from string using regular expressions\n",
    "def extract_phone_numbers(string):\n",
    "    r = re.compile(r'(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})')\n",
    "    phone_numbers = r.findall(string)\n",
    "    return [re.sub(r'\\D', '', number) for number in phone_numbers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to extract Email address from a string using regular expressions\n",
    "def extract_email_addresses(string):\n",
    "    r = re.compile(r'[\\w\\.-]+@[\\w\\.-]+')\n",
    "    return r.findall(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting pdf to string\n",
    "resume_string = convert(\"resume.pdf\")\n",
    "resume_string1 = resume_string\n",
    "#Removing commas in the resume for an effecient check\n",
    "resume_string = resume_string.replace(',',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting all the charachters in lower case\n",
    "resume_string = resume_string.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Information Extraction Function\n",
    "def extract_information(string):\n",
    "    string.replace (\" \", \"+\")\n",
    "    query = string\n",
    "    soup = BeautifulSoup(urlopen(\"https://en.wikipedia.org/wiki/\" + query), \"html.parser\")\n",
    "    #creates soup and opens URL for Google. Begins search with site:wikipedia.com so only wikipedia\n",
    "    #links show up. Uses html parser.\n",
    "    for item in soup.find_all('div', attrs={'id' : \"mw-content-text\"}):\n",
    "        print(item.find('p').get_text())\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('techatt.csv', 'rt', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    your_listatt = list(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('techskill.csv', 'rt', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    your_list = list(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nontechnicalskills.csv', 'rt', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    your_list1 = list(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Sets are used as it has a a constant time for lookup hence the overall the time for the total code will not exceed O(n)\n",
    "s = set(your_list[0])\n",
    "s1 = your_list\n",
    "s2 = your_listatt\n",
    "skillindex = []\n",
    "skills = []\n",
    "skillsatt = []\n",
    "print('\\n')\n",
    "extract_name(resume_string1)\n",
    "print('\\n')\n",
    "\n",
    "y = extract_phone_numbers(resume_string) #Phone Number as in resume\n",
    "y1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y)):\n",
    "    if(len(y[i])>9):\n",
    "        y1.append(y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['9933995693']\n",
      "\n",
      "\n",
      "Email id is\n",
      "['ashaywalke@iitkgp.ac.in']\n"
     ]
    }
   ],
   "source": [
    "print(y1)\n",
    "print('\\n')\n",
    "print('Email id is')\n",
    "print(extract_email_addresses(resume_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in resume_string.split(\" \"):\n",
    "    if word in s:\n",
    "        skills.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "skills1 = list(set(skills))\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "np_a1 = np.array(your_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(skills1)):\n",
    "    item_index = np.where(np_a1==skills1[i])\n",
    "    skillindex.append(item_index[1][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlen = len(skillindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Following are his/her Technical Skills\n",
      "\n",
      "\n",
      "algorithms\n",
      "Algorithms and Design Patterns\n",
      "\n",
      "\n",
      "numpy\n",
      "Mathematical Functions Library\n",
      "\n",
      "\n",
      "c\n",
      "Programming Language\n",
      "\n",
      "\n",
      "python\n",
      "Programmig Language\n",
      "\n",
      "\n",
      "opencv\n",
      "Computer Vision\n",
      "\n",
      "\n",
      "scikit-learn\n",
      "Machine Learning\n",
      "\n",
      "\n",
      "matplotlib\n",
      "Data Visualization\n",
      "\n",
      "\n",
      "c++\n",
      "Programming Language\n",
      "\n",
      "\n",
      "matlab\n",
      "Programming Language\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Following are his/her Technical Skills\\n\\n\")\n",
    "\n",
    "for i in range(nlen):\n",
    "    print(skills1[i])\n",
    "    print(s2[0][skillindex[i]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sets are used as it has a a constant time for lookup hence the overall the time for the total code will not exceed O(n)\n",
    "s1 = set(your_list1[0])\n",
    "nontechskills = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in resume_string.split(\" \"):\n",
    "    if word in s1:\n",
    "        nontechskills.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Following are his/her Non Technical Skills\n",
      "\n",
      "\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "nontechskills = set(nontechskills)\n",
    "print('\\n')\n",
    "\n",
    "print(\"Following are his/her Non Technical Skills\")\n",
    "list5 = list(nontechskills)\n",
    "print('\\n')\n",
    "print(list5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
