{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, absolute_import\n",
    "import re\n",
    "import numpy as np\n",
    "import tflearn\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected, flatten\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d, avg_pool_2d\n",
    "from tflearn.layers.merge_ops import merge\n",
    "from tflearn.layers.normalization import local_response_normalization\n",
    "from tflearn.layers.estimator import regression\n",
    "from os.path import isfile, join\n",
    "import random\n",
    "import sys \n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMR:\n",
    "\n",
    "  def __init__(self):\n",
    "  \tself.target_classes = ['angry', 'disgusted', 'fearful', 'happy', 'sad', 'surprised', 'neutral']\n",
    "\n",
    "  def build_network(self):\n",
    "      \"\"\"\n",
    "      Build the convnet.\n",
    "      Input is 48x48\n",
    "      3072 nodes in fully connected layer\n",
    "      \"\"\" \n",
    "      print(\"\\n---> Starting Neural Network \\n\") \n",
    "      self.network = input_data(shape = [None, 48, 48, 1])\n",
    "      print(\"Input data\",self.network.shape[1:])\n",
    "      self.network = conv_2d(self.network, 64, 5, activation = 'relu')\n",
    "      print(\"Conv1\",self.network.shape[1:])\n",
    "      self.network = max_pool_2d(self.network, 3, strides = 2)\n",
    "      print(\"Maxpool\",self.network.shape[1:])\n",
    "      self.network = conv_2d(self.network, 64, 5, activation = 'relu')\n",
    "      print(\"Conv2\",self.network.shape[1:])\n",
    "      self.network = max_pool_2d(self.network, 3, strides = 2)\n",
    "      print(\"Maxpool2\",self.network.shape[1:])\n",
    "      self.network = conv_2d(self.network, 128, 4, activation = 'relu')\n",
    "      print(\"Conv3\",self.network.shape[1:])\n",
    "      self.network = dropout(self.network, 0.3)\n",
    "      print(\"Dropout\",self.network.shape[1:])\n",
    "      self.network = fully_connected(self.network, 3072, activation = 'relu')\n",
    "      print(\"Fully connected\",self.network.shape[1:])\n",
    "      self.network = fully_connected(self.network, len(self.target_classes), activation = 'softmax')\n",
    "      print(\"Output\",self.network.shape[1:])\n",
    "      print('\\n')\n",
    "      # Generates a TrainOp which contains the information about optimization process - optimizer, loss function, etc\n",
    "      self.network = regression(self.network,optimizer = 'momentum',metric = 'accuracy',loss = 'categorical_crossentropy')\n",
    "      # Creates a model instance.\n",
    "      self.model = tflearn.DNN(self.network,checkpoint_path = 'model_1_atul',max_checkpoints = 1,tensorboard_verbose = 2)\n",
    "      # Loads the model weights from the checkpoint\n",
    "      self.load_model()\n",
    "\n",
    "  def predict(self, image):\n",
    "    \"\"\"\n",
    "    Image is resized to 48x48, and predictions are returned.\n",
    "    \"\"\"\n",
    "    if image is None:\n",
    "      return None\n",
    "    image = image.reshape([-1, 48, 48, 1])\n",
    "    return self.model.predict(image)\n",
    "\n",
    "  def load_model(self):\n",
    "    \"\"\"\n",
    "    Loads pre-trained model.\n",
    "    \"\"\"\n",
    "    if isfile(\"model_1_atul.tflearn.meta\"):\n",
    "      self.model.load(\"model_1_atul.tflearn\", weights_only=True)\n",
    "      print('\\n---> Pre-trained model loaded')\n",
    "    else:\n",
    "        print(\"---> Couldn't find model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\n\\nif __name__ == \"__main__\":\\n  print(\"\\n------------Emotion Detection Program------------\\n\")\\n  network = EMR()\\n  if sys.argv[1] == \\'singleface\\':\\n    import singleface\\n    print(\\'In singleface\\')\\n  if sys.argv[1] == \\'multiface\\':\\n    import multiface\\n    \\n    \\n    \\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  print(\"\\n------------Emotion Detection Program------------\\n\")\n",
    "  network = EMR()\n",
    "  if sys.argv[1] == 'singleface':\n",
    "    import singleface\n",
    "    print('In singleface')\n",
    "  if sys.argv[1] == 'multiface':\n",
    "    import multiface\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMOTIONS = ['angry', 'disgusted', 'fearful', 'happy', 'sad', 'surprised', 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the cascade\n",
    "cascade_classifier = cv2.CascadeClassifier('haarcascade_files/haarcascade_frontalface_default.xml')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_image(image):\n",
    "    \"\"\"\n",
    "    Function to format frame\n",
    "    \"\"\"\n",
    "    if len(image.shape) > 2 and image.shape[2] == 3:\n",
    "        # determine whether the image is color\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        # Image read from buffer\n",
    "        image = cv2.imdecode(image, cv2.CV_LOAD_IMAGE_GRAYSCALE)\n",
    "\n",
    "    faces = cascade_classifier.detectMultiScale(image,scaleFactor = 1.3 ,minNeighbors = 5)\n",
    "\n",
    "    if not len(faces) > 0:\n",
    "        return None\n",
    "\n",
    "    # initialize the first face as having maximum area, then find the one with max_area\n",
    "    max_area_face = faces[0]\n",
    "    for face in faces:\n",
    "        if face[2] * face[3] > max_area_face[2] * max_area_face[3]:\n",
    "            max_area_face = face\n",
    "    face = max_area_face\n",
    "\n",
    "    # extract ROI of face\n",
    "    image = image[face[1]:(face[1] + face[2]), face[0]:(face[0] + face[3])]\n",
    "\n",
    "    try:\n",
    "        # resize the image so that it can be passed to the neural network\n",
    "        image = cv2.resize(image, (48,48), interpolation = cv2.INTER_CUBIC) / 255.\n",
    "    except Exception:\n",
    "        print(\"----->Problem during resize\")\n",
    "        return None\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> Starting Neural Network \n",
      "\n",
      "Input data (48, 48, 1)\n",
      "WARNING:tensorflow:From C:\\Users\\Home\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tflearn\\initializations.py:119: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
      "Conv1 (48, 48, 64)\n",
      "Maxpool (24, 24, 64)\n",
      "Conv2 (24, 24, 64)\n",
      "Maxpool2 (12, 12, 64)\n",
      "Conv3 (12, 12, 128)\n",
      "Dropout (12, 12, 128)\n",
      "Fully connected (3072,)\n",
      "Output (7,)\n",
      "\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Home\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\tflearn\\objectives.py:66: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\Home\\upwork_project_list\\human_face_emotion_classification_in_python\\model_1_atul.tflearn\n",
      "\n",
      "---> Pre-trained model loaded\n"
     ]
    }
   ],
   "source": [
    "# Initialize object of EMR class\n",
    "network = EMR()\n",
    "network.build_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(\"human_face_emotion.mp4\")\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "feelings_faces = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the list with the emoji images\n",
    "for index, emotion in enumerate(EMOTIONS):\n",
    "    feelings_faces.append(cv2.imread('./emojis/' + emotion + '.png', -1))\n",
    "    \n",
    "    #\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "# press-   q    to exit. \n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(3.4.4) C:\\projects\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:181: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-8f7ceb0d335b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mfacecasc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCascadeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'haarcascade_files/haarcascade_frontalface_default.xml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mgray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mfaces\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfacecasc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(3.4.4) C:\\projects\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:181: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # Again find haar cascade to draw bounding box around face\n",
    "    ret, frame = cap.read()\n",
    "    facecasc = cv2.CascadeClassifier('haarcascade_files/haarcascade_frontalface_default.xml')\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = facecasc.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    # compute softmax probabilities\n",
    "    result = network.predict(format_image(frame))\n",
    "    if result is not None:\n",
    "        # write the different emotions and have a bar to indicate probabilities for each class\n",
    "        for index, emotion in enumerate(EMOTIONS):\n",
    "            cv2.putText(frame, emotion, (10, index * 20 + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1);\n",
    "            cv2.rectangle(frame, (130, index * 20 + 10), (130 + int(result[0][index] * 100), (index + 1) * 20 + 4), (255, 0, 0), -1)\n",
    "\n",
    "        # find the emotion with maximum probability and display it\n",
    "        maxindex = np.argmax(result[0])\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        cv2.putText(frame,EMOTIONS[maxindex],(10,360), font, 2,(255,255,255),2,cv2.LINE_AA) \n",
    "        face_image = feelings_faces[maxindex]\n",
    "        print(face_image[:,:,3])\n",
    "\n",
    "        for c in range(0, 3):\n",
    "            # the shape of face_image is (x,y,4)\n",
    "            # the fourth channel is 0 or 1\n",
    "            # in most cases it is 0, so, we assign the roi to the emoji\n",
    "            # you could also do:\n",
    "            # frame[200:320,10:130,c] = frame[200:320, 10:130, c] * (1.0 - face_image[:, :, 3] / 255.0)\n",
    "            frame[200:320, 10:130, c] = face_image[:,:,c] * (face_image[:, :, 3] / 255.0) +  frame[200:320, 10:130, c] * (1.0 - face_image[:, :, 3] / 255.0)\n",
    "\n",
    "    if not len(faces) > 0:\n",
    "        # do nothing if no face is detected\n",
    "        a = 1\n",
    "    else:\n",
    "        # draw box around face with maximum area\n",
    "        max_area_face = faces[0]\n",
    "        for face in faces:\n",
    "            if face[2] * face[3] > max_area_face[2] * max_area_face[3]:\n",
    "                max_area_face = face\n",
    "        face = max_area_face\n",
    "        (x,y,w,h) = max_area_face\n",
    "        frame = cv2.rectangle(frame,(x,y-50),(x+w,y+h+10),(255,0,0),2)\n",
    "\n",
    "    cv2.imshow('Video', cv2.resize(frame,None,fx=2,fy=2,interpolation = cv2.INTER_CUBIC))\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
